# 📦 Libraries
library(tidyverse)
library(readr)
library(fastDummies)
library(glmnet)
library(Metrics)      # For R^2 calculation
library(DataExplorer) # For EDA (optional)
library(corrplot)     # For correlation plotting

# 📁 Load datasets
train_target     <- read_csv("~/Desktop/MDA Kaggle Comp. Data/Kaggle Competition/Train_Target.csv")
train_predictor  <- read_csv("~/Desktop/MDA Kaggle Comp. Data/Kaggle Competition/Training_Data_Predictors.csv")
test_predictor   <- read_csv("~/Desktop/MDA Kaggle Comp. Data/Kaggle Competition/Test_Data_Predictors.csv")

# ------------------------------------------------------------------------------
# Data Preprocessing & Feature Engineering
# ------------------------------------------------------------------------------

# Create dummy variables for 'Type'
train_predictor <- train_predictor %>%
  mutate(Type_Photo  = ifelse(Type == "Photo", 1, 0),
         Type_Link   = ifelse(Type == "Link", 1, 0),
         Type_Status = ifelse(Type == "Status", 1, 0)) %>%
  dplyr::select(-Type)

test_predictor <- test_predictor %>%
  mutate(Type_Photo  = ifelse(Type == "Photo", 1, 0),
         Type_Link   = ifelse(Type == "Link", 1, 0),
         Type_Status = ifelse(Type == "Status", 1, 0)) %>%
  dplyr::select(-Type)

# Create dummy variables for 'Category'
train_predictor <- train_predictor %>%
  mutate(Category_1 = ifelse(Category == 1, 1, 0),
         Category_2 = ifelse(Category == 2, 1, 0)) %>%
  dplyr::select(-Category)

test_predictor <- test_predictor %>%
  mutate(Category_1 = ifelse(Category == 1, 1, 0),
         Category_2 = ifelse(Category == 2, 1, 0)) %>%
  dplyr::select(-Category)
numeric_cols <- train_predictor %>% select_if(is.numeric) %>% names()
dummy_cols   <- c("Type_Photo","Type_Link","Type_Status","Category_1","Category_2","Paid")
cols_to_log  <- setdiff(numeric_cols, dummy_cols)

train_predictor[cols_to_log] <- lapply(train_predictor[cols_to_log], log1p)
test_predictor[cols_to_log]  <- lapply(test_predictor[cols_to_log],  log1p)

# Remove "ID" columns if present
if ("ID" %in% names(train_target)) {
  train_target <- train_target %>% dplyr::select(-ID)
}
if ("ID" %in% names(train_predictor)) {
  train_predictor <- train_predictor %>% dplyr::select(-ID)
  test_predictor  <- test_predictor %>% dplyr::select(-ID)
}

# Merge target and predictors into one training dataset and remove any missing values
full_train <- cbind(train_target, train_predictor) %>% na.omit()

# Determine target column name (assumes only one column in train_target)
target_col <- names(train_target)[1]

# ------------------------------------------------------------------------------
# Modeling: Degree-3 Polynomial Expansion & Feature Selection
# ------------------------------------------------------------------------------

# Use a degree-3 polynomial expansion (all main effects, squared, cubic, and interactions up to order 3)
formula_deg3 <- as.formula(paste0(target_col, " ~ .^3"))
x_train_full <- model.matrix(formula_deg3, data = full_train)[, -1]
y_train <- full_train[[target_col]]

# Create the degree-3 polynomial design matrix for the test set
x_test_full <- model.matrix(~ .^3, data = test_predictor)[, -1]

# ------------------------------------------------------------------------------
# Feature Selection with Lasso (α = 1)
# ------------------------------------------------------------------------------
set.seed(123)
cv_lasso <- cv.glmnet(x_train_full, y_train, alpha = 1, standardize = TRUE)
best_lambda_lasso <- cv_lasso$lambda.min

lasso_model <- glmnet(x_train_full, y_train, alpha = 1, lambda = best_lambda_lasso, standardize = TRUE)
lasso_coefs <- coef(lasso_model)

# Identify features with non-zero coefficients (excluding the intercept)
selected_features <- rownames(lasso_coefs)[which(lasso_coefs != 0)]
selected_features <- selected_features[selected_features != "(Intercept)"]

cat("Number of selected polynomial (degree-3) features:", length(selected_features), "\n")

# Subset the design matrices to include only the selected features
x_train_selected <- x_train_full[, selected_features, drop = FALSE]
x_test_selected <- x_test_full[, selected_features, drop = FALSE]

# ------------------------------------------------------------------------------
# Final Model: Ridge Regression on Selected Features (α = 0)
# ------------------------------------------------------------------------------
set.seed(123)
cv_ridge <- cv.glmnet(x_train_selected, y_train, alpha = 0, standardize = TRUE)
best_lambda_ridge <- cv_ridge$lambda.min

ridge_model <- glmnet(x_train_selected, y_train, alpha = 0, lambda = best_lambda_ridge, standardize = TRUE)

y_pred_train <- predict(ridge_model, s = best_lambda_ridge, newx = x_train_selected)
y_pred_train <- as.vector(y_pred_train)

# 2) Compute residuals, n and p
residuals <- y_train - y_pred_train
n         <- length(y_train)
p         <- length(selected_features) + 1   # +1 to count the intercept

# 3) Sum of squared residuals & total sum of squares
RSS <- sum(residuals^2)
TSS <- sum((y_train - mean(y_train))^2)

# 4) R² and adjusted R²
R2   <- 1 - RSS / TSS
adjR2 <- 1 - (1 - R2) * (n - 1) / (n - p - 1)

# 5) AIC and BIC (using the usual “n·log(RSS/n) + penalty·k” form)
AIC_val <- n * log(RSS / n) + 2 * p
BIC_val <- n * log(RSS / n) + log(n) * p

# 6) Print out the results
cat("Training set metrics:\n")
cat("  Adjusted R²:", round(adjR2, 4), "\n")
cat("  AIC:        ", round(AIC_val, 2), "\n")
cat("  BIC:        ", round(BIC_val, 2), "\n")
# ------------------------------------------------------------------------------
# Prediction on Test Set & Export Results
# ------------------------------------------------------------------------------
test_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test_selected)
predictions <- tibble(ID = 1:nrow(test_predictor), .pred = as.vector(test_predictions))
write.csv(predictions, "Mk8.csv", row.names = FALSE)
